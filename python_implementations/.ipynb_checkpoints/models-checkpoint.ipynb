{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    :param x: Inputs, of any shape\n",
    "    :return: A tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x for back-propagation\n",
    "    \"\"\"\n",
    "    out = np.zeros_like(x)\n",
    "    out[np.where(x > 0)] = x[np.where(x > 0)]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    \n",
    "    :param x: (float) a tensor of shape (N, #classes)\n",
    "    \"\"\"\n",
    "    delta = np.max(x, axis=1)\n",
    "    x -= delta\n",
    "    exp_x = np.exp(x)\n",
    "    sumup = np.sum(exp_x, axis=1)\n",
    "    \n",
    "    result = exp_x / sumup\n",
    "    return result\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    :param x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    :param w: A numpy array of weights, of shape (D, M)\n",
    "    :param b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    :return:\n",
    "    - out: output, of shape (N, M)\n",
    "    \"\"\"\n",
    "    num_train = x.shape[0]\n",
    "    x_flatten = x.reshape((num_train, -1))\n",
    "    out = np.dot(x_flatten, w) + b\n",
    "    \n",
    "    return out\n",
    "\n",
    "def SVM(x, w, b):\n",
    "    \"\"\"\n",
    "    x: (N, dim)\n",
    "    w: weights\n",
    "    b: bias\n",
    "    \n",
    "    return:\n",
    "        the result lable\n",
    "    \"\"\"\n",
    "    output_FM = affine_forward(x, w, b)\n",
    "    \n",
    "    if output_FM > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def logistic_regression(x, w, b):\n",
    "    \"\"\"\n",
    "    x: (N, dim)\n",
    "    w: weights\n",
    "    b: bias\n",
    "    \n",
    "    return:\n",
    "        the result lable\n",
    "    \"\"\"\n",
    "    output_FM = relu_forward(affine_forward(x, w, b))\n",
    "    \n",
    "    # if we don't want to know the probability, we can remove softmax\n",
    "    result = np.argmax(softmax(output_FM))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def neural_networks(x, weights, biases):\n",
    "    \"\"\"\n",
    "    x: (N, dim)\n",
    "    weights: list of weights, [W1, W2, W3 ...]\n",
    "    biases: list of biases, [b1, b2, b3 ...]\n",
    "    \n",
    "    return:\n",
    "        the result lable\n",
    "    \"\"\"\n",
    "    input_FM = x\n",
    "    assert len(weights) == len(biases)\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        weight = weights[i]\n",
    "        bias = biases[i]\n",
    "        output_FM = relu_forward(affine_forward(input_FM, weight, bias))\n",
    "        input_FM = output_FM\n",
    "        \n",
    "    # if we don't want to know the probability, we can remove softmax\n",
    "    result = np.argmax(softmax(output_FM))\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
